# ОТЧЕТ

## 1. Измерение времени работы изначальной программы
За основу возьмем изначальный код, в который добавим возможность генерации кастомного поля.
Размер поля для тестов будет 100 на 300
Процессор 1th Gen Intel(R) Core(TM) i7-1165G7 @ 2.80GHz 2.80 GHz
Количество Тиков 100:
1) 166.537 секунды
2) 146.824 секунды
3) 136.983 секунды
4) 137.592 секунды
5) 113.926 секунды

Среднее время работы в одном потоке: 140.3724 секунд

### Первичная оптимизация

В первичную оптимизацию можно выделить следующие пункты:

1. **Замена рандома:**
   - Если понаглеть и жестко влезть в рамки ТЗ, то мы можем вместо рандома всегда возвращать 0.0, т.к по требованиям(корректность работы с точностью до рандома НЕ включительно) все будет работать правильно, но это ускорит нашу программу в разы. Так же этим действием мы не отключаем другой функционал, потому что этот 0.0 просто увеличивает наши шансы быстрее упасть, но на все остальное так же остаются шансы. Так же это сделает наши тесты более точными, потому что влияние рандома будет очень маленьким(но оно будет)

2. **Оптимизация циклов:**
   - Осимптотика основного цикла, без учета других методов, равна `O(T * N * M)`.
   - Так как первый и последний ряд мы пропускаем полностью (они состоят только из `#`), можно сдвинуть границы циклов с `[0; N - 1]` на `[1; N - 2]`, что дает нам константный прирост производительности с хорошей константой. Подразумевается, что мы меняем все циклы  for (size_t x = 0; x < N; ++x) на 
   for (size_t x = 1; x < N - 1; ++x)

3. **Удаление `total_delta_p`:**
   - Выкидываем все строки с этой переменной, потому что она нигде не используется, но требует сложных вычислений и кастов типов.
   ```cpp
    total_delta_p -= force / dirs[x][y]; 
    total_delta_p += force / dirs[x + dx][y + dy];
    total_delta_p += force / dirs[x][y];
   ```

После оптимизаций время работы программы:
1) 83.09 секунд
2) 73.9578 секунд
3) 77.9497 секунд
4) 74.013 секунд
5) 72.7974 секунд


Среднее время работы в 1 потоке после оптимизации: 76,3616 секунд

Наша оптимизация дала прирост в 45.61% от исходного времени работы в 1 потоке.


### **Часть 2: Распараллеливание**
---

## 1. Распараллеливание

1. **Чтение количества потоков:**
   - Добавляем считываение потоков из стандартного ввода и записываем его в переменную numThreads

2. **Параллелизация обхода матрицы:**
В программе есть шесть циклов, которые обходят матрицу
   
   **Пример параллельного цикла:**
   ```cpp
   for (size_t x = 1; x < N - 1; ++x) {
       for (size_t y = 0; y < M; ++y) {
           if (field[x][y] == '#')
               continue;
           for (auto [dx, dy] : deltas) {
               dirs[x][y] += (field[x + dx][y + dy] != '#');
           }
       }
   }

    for (size_t x = 1; x < N - 1; ++x) {
        for (size_t y = 0; y < M; ++y) {
            if (field[x][y] == '#') continue;
            if (field[x + 1][y] != '#')
                velocity.add(x, y, 1, 0, g_);
        }
    }


    for (size_t x = 1; x < N - 1; ++x) {
        for (size_t y = 0; y < M; ++y) {
            if (field[x][y] == '#')
                continue;
            for (auto [dx, dy] : deltas) {
                int nx = x + dx, ny = y + dy;
                if (field[nx][ny] != '#' && old_p[nx][ny] < old_p[x][y]) {
                    auto delta_p = old_p[x][y] - old_p[nx][ny];
                    auto force = delta_p;
                    auto &contr = velocity.get(nx, ny, -dx, -dy);
                    if (force <= contr * rho_[(int) field[nx][ny]]) {
                        contr -= static_cast<VType>(static_cast<VType>(force) / rho_[(int) field[nx][ny]]);
                        continue;
                    }
                    force -= contr * rho_[(int) field[nx][ny]];
                    contr = 0;
                    velocity.add(x, y, dx, dy, static_cast<VType>(force) / rho_[(int) field[x][y]]);
                    p[x][y] -= force / dirs[x][y];
                }
            }
        }
    }

    for (size_t x = 1; x < N - 1; ++x) {
        for (size_t y = 0; y < M; ++y) {
            if (field[x][y] == '#')
                continue;
            for (auto [dx, dy] : deltas) {
                auto old_v = velocity.get(x, y, dx, dy);
                auto new_v = velocity_flow.get(x, y, dx, dy);
                if (old_v > 0) {
                    
                    velocity.get(x, y, dx, dy) = static_cast<VType>(new_v);
                    auto force = (static_cast<VFlowType>(old_v) - new_v) * rho_[(int) field[x][y]];
                    if (field[x][y] == '.')
                        force *= 0.8;
                    if (field[x + dx][y + dy] == '#') {
                        p[x][y] += force / dirs[x][y];
                    } else {
                        p[x + dx][y + dy] += force / dirs[x + dx][y + dy];
                    }
                }
            }
        }
    }
    ```
---
Эти три мы можем спокойно параллелить, потому что в они никак не ломают нашу программу. Для этого мы используем пулл потоков, который предоставляет нам openMP


    ```cpp
    for (size_t x = 1; x < N - 1; ++x) {
        for (size_t y = 0; y < M; ++y) {
            if (field[x][y] != '#' && last_use[x][y] != UT) {
                    auto [t, local_prop, _] = propagate_flow(x, y, 1);
                    if (t > 0) {
                        prop = true;
                }
                
            }
        }
    }
    for (size_t x = 1; x < N; ++x) {
        for (size_t y = 0; y < M; ++y) {
            if (field[x][y] != '#' && last_use[x][y] != UT) {
                if (move_prob(x, y) > random01()) {
                    prop = true;
                    propagate_move(x, y, true);
                } else {
                    propagate_stop(x, y, true);
                }
            }
        }
    }
    ```

---
Эти два цикла мы уже не можем просто так расспаралеллить, потому что они управляют глобальными данными, что вызывает race condition, что триггерит assert и ломает нашу программу. Есть 3 способа как избежать этого - использовать mutex, critical, или не параллелить первый цикл вовсе. Я быбрал 3 вариант, не смотря на то, что critcal работает более эффективно, чем mutex, он все равно негативно влияет на первофанс, потому что нужно навешивать critical на каждый вызов функций, что сильно замедлит нашу программу. Поэтому мы можем расспаралелить только второй цикл, что даст нам наибольшую выгоду, потому что мы вообще не останавливаем работу потоков.

Замеры для двух потоков
1) 74.5175
2) 73.5634
3) 71.9843
4) 73.1237
5) 75.0895

Среднее время работы в 2 потоках: 73.655 секунд
Это дает нам ускорение в 3.5% от исходного времени работы. Что в принципе ожидаемо, потому что у нас достаточно маленький тест.

Замеры для 6 потоков
1) 71.8868
2) 72.1561
3) 72.1057
4) 70.2042
5) 71.6431

Среднее время работы в 6 потоках: 71.5991
Это дает нам ускорение в 6.2% относительно однопоточного выполнения программы.

Для 8 потоков в среднем результат получается равным 72.2 секундам, что дает ускорение в 5.4%. Это объяснимо тем, что для матрицы таких размеров 8 потоков это слишком много, и мы больше тратим на управление ими, чем получаем профит от них.


### **Часть 3: Итог** 
 - Добавлена возможнсть выбора количества потоков для работы программы.
 - Добавлена возможность расспаралеливания программы с корректной логикой выполнения.
 - Оптимизация работы в однопоточном режиме на 45.61%.
 - Оптимизация работы в параллельном режиме для оптимизированного кода на 3.5% и 6.2% для 2 и 6 потоков     соответственно.
 - Оптимизация работы в параллельном режиме относительно неоптимезированного кода на 48.4% для 8 потоков.


---
